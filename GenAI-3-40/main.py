import json
import re
from typing import Dict, List, Any, Tuple
from datetime import datetime
import random
import torch
from transformers import (
    AutoTokenizer, 
    AutoModelForCausalLM,
    pipeline,
    set_seed
)
import nltk

nltk.download('vader_lexicon')
from nltk.sentiment import SentimentIntensityAnalyzer
import logging

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class LocalContentGenerator:
    def __init__(self, model_name: str = "IlyaGusev/rut5_base"):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        self.model_name = model_name
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        logger.info(f"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {self.device}")
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
        self.topics = self._initialize_topics()
        self.sentiment_analyzer = SentimentIntensityAnalyzer()
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞
        try:
            logger.info("–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞...")
            self.text_generator = pipeline(
                "text-generation",
                model=model_name,
                tokenizer=model_name,
                device=0 if self.device == "cuda" else -1
            )
            logger.info("–ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
            self.text_generator = None
    
    def _initialize_topics(self) -> List[str]:
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–ø–∏—Å–∫–∞ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö —Ç–µ–º"""
        return [
            "–ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç –≤ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–º –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–∏: –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –∏ –≤—ã–∑–æ–≤—ã",
            "–í–æ–∑–æ–±–Ω–æ–≤–ª—è–µ–º–∞—è —ç–Ω–µ—Ä–≥–µ—Ç–∏–∫–∞ –∏ —É—Å—Ç–æ–π—á–∏–≤–æ–µ —Ä–∞–∑–≤–∏—Ç–∏–µ –≥–æ—Ä–æ–¥–æ–≤",
            "–ö–≤–∞–Ω—Ç–æ–≤—ã–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è: —Ä–µ–≤–æ–ª—é—Ü–∏—è –≤ –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω—ã—Ö —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è—Ö",
            "–ë–∏–æ—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ –∏ –≥–µ–Ω–Ω–∞—è –∏–Ω–∂–µ–Ω–µ—Ä–∏—è –≤ –º–µ–¥–∏—Ü–∏–Ω–µ –±—É–¥—É—â–µ–≥–æ",
            "–ö–æ—Å–º–∏—á–µ—Å–∫–∏–π —Ç—É—Ä–∏–∑–º –∏ –æ—Å–≤–æ–µ–Ω–∏–µ –∫–æ—Å–º–æ—Å–∞ —á–∞—Å—Ç–Ω—ã–º–∏ –∫–æ–º–ø–∞–Ω–∏—è–º–∏",
            "–¶–∏—Ñ—Ä–æ–≤–∞—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è –±–∏–∑–Ω–µ—Å–∞ –≤ –ø–æ—Å—Ç–ø–∞–Ω–¥–µ–º–∏–π–Ω—É—é —ç–ø–æ—Ö—É",
            "–£–º–Ω—ã–µ –≥–æ—Ä–æ–¥–∞ –∏ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç –≤–µ—â–µ–π: —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ –¥–ª—è –∫–æ–º—Ñ–æ—Ä—Ç–Ω–æ–π –∂–∏–∑–Ω–∏",
            "–ù–µ–π—Ä–æ—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ –∏ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å—ã –º–æ–∑–≥-–∫–æ–º–ø—å—é—Ç–µ—Ä",
            "–ë–ª–æ–∫—á–µ–π–Ω –∏ –¥–µ—Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ —Ñ–∏–Ω–∞–Ω—Å—ã (DeFi)",
            "–†–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–∞ –∏ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è –≤ –ø—Ä–æ–º—ã—à–ª–µ–Ω–Ω–æ—Å—Ç–∏ 4.0"
        ]
    
    def select_topic(self) -> str:
        """1. –í—ã–±–æ—Ä —Ç–µ–º—ã"""
        topic = random.choice(self.topics)
        logger.info(f"–í—ã–±—Ä–∞–Ω–∞ —Ç–µ–º–∞: {topic}")
        return topic
    
    def analyze_topic(self, topic: str) -> Dict[str, Any]:
        """2. –ê–Ω–∞–ª–∏–∑ —Ç–µ–∫—Å—Ç–æ–≤ –ø–æ —Ç–µ–º–µ (–∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞, —Å—Ç–∏–ª—å)"""
        logger.info("–ü—Ä–æ–≤–µ–¥–µ–Ω–∏–µ –∞–Ω–∞–ª–∏–∑–∞ —Ç–µ–º—ã...")
        
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –∏–∑ —Ç–µ–º—ã
        keywords = self._extract_keywords(topic)
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å—Ç–∏–ª—è –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–º—ã
        style = self._determine_style(topic)
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏
        tone = self._determine_tone(topic)
        
        analysis = {
            "keywords": keywords,
            "style": style,
            "tone": tone,
            "target_audience": self._determine_audience(topic),
            "complexity": self._assess_complexity(topic)
        }
        
        logger.info(f"–ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω: {len(keywords)} –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤, —Å—Ç–∏–ª—å: {style}")
        return analysis
    
    def _extract_keywords(self, topic: str) -> List[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –∏–∑ —Ç–µ–º—ã"""
        # –£–¥–∞–ª—è–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤–∞ –∏ –≤—ã–¥–µ–ª—è–µ–º –∑–Ω–∞—á–∏–º—ã–µ —Å–ª–æ–≤–∞
        stop_words = {"–≤", "–∏", "–Ω–∞", "—Å", "–ø–æ", "–¥–ª—è", "–∏–∑", "–æ—Ç", "–¥–æ", "–∑–∞", "–Ω–µ", "—á—Ç–æ", "–∫–∞–∫"}
        words = re.findall(r'\b[–∞-—è—ë]+\b', topic.lower())
        keywords = [word for word in words if word not in stop_words and len(word) > 2]
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–º—ã
        related_terms = self._get_related_terms(topic)
        keywords.extend(related_terms)
        
        return list(set(keywords))  # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
    
    def _get_related_terms(self, topic: str) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–º—ã"""
        topic_lower = topic.lower()
        
        related_terms_map = {
            "–∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç": ["–º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ", "–Ω–µ–π—Ä–æ—Å–µ—Ç–∏", "ai", "–∞–ª–≥–æ—Ä–∏—Ç–º—ã", "–¥–∞–Ω–Ω—ã–µ"],
            "–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ": ["–æ–±—É—á–µ–Ω–∏–µ", "—Å—Ç—É–¥–µ–Ω—Ç—ã", "–ø—Ä–µ–ø–æ–¥–∞–≤–∞—Ç–µ–ª–∏", "—É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç", "–∫—É—Ä—Å—ã"],
            "—ç–Ω–µ—Ä–≥–µ—Ç–∏–∫–∞": ["—ç–Ω–µ—Ä–≥–∏—è", "—Å–æ–ª–Ω–µ—á–Ω–∞—è", "–≤–µ—Ç—Ä–æ–≤–∞—è", "—ç–ª–µ–∫—Ç—Ä–æ—Å—Ç–∞–Ω—Ü–∏—è", "—ç–∫–æ–ª–æ–≥–∏—è"],
            "–∫–≤–∞–Ω—Ç–æ–≤—ã–µ": ["–∫–≤–∞–Ω—Ç", "–≤—ã—á–∏—Å–ª–µ–Ω–∏—è", "–∫—É–±–∏—Ç—ã", "—Å—É–ø–µ—Ä–ø–æ–∑–∏—Ü–∏—è", "–∑–∞–ø—É—Ç—ã–≤–∞–Ω–∏–µ"],
            "–±–∏–æ—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏": ["–≥–µ–Ω—ã", "–¥–Ω–∫", "–∫–ª–µ—Ç–∫–∏", "—Ç–µ—Ä–∞–ø–∏—è", "–≥–µ–Ω–µ—Ç–∏–∫–∞"],
            "–∫–æ—Å–º–∏—á–µ—Å–∫–∏–π": ["–∫–æ—Å–º–æ—Å", "—Ä–∞–∫–µ—Ç—ã", "–æ—Ä–±–∏—Ç–∞", "—Å–ø—É—Ç–Ω–∏–∫–∏", "–Ω–∞—Å–∞"],
            "—Ü–∏—Ñ—Ä–æ–≤–∞—è": ["digital", "–æ–Ω–ª–∞–π–Ω", "–∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è", "—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏", "–∏–Ω–Ω–æ–≤–∞—Ü–∏–∏"],
            "—É–º–Ω—ã–µ –≥–æ—Ä–æ–¥–∞": ["iot", "—Å–µ–Ω—Å–æ—Ä—ã", "–∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä–∞", "—Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç", "–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å"],
            "–Ω–µ–π—Ä–æ—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏": ["–º–æ–∑–≥", "–Ω–µ–π—Ä–æ–Ω—ã", "—Å–∏–≥–Ω–∞–ª—ã", "–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å", "–∫–æ–≥–Ω–∏—Ç–∏–≤–Ω—ã–π"],
            "–±–ª–æ–∫—á–µ–π–Ω": ["–∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç–∞", "—Å–º–∞—Ä—Ç-–∫–æ–Ω—Ç—Ä–∞–∫—Ç—ã", "–¥–µ—Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–∞—Ü–∏—è", "–±–∏—Ç–∫–æ–∏–Ω", "—ç—Ñ–∏—Ä–∏—É–º"],
            "—Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–∞": ["—Ä–æ–±–æ—Ç—ã", "–∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è", "–ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–æ", "–∏–∏", "–º–∞–Ω–∏–ø—É–ª—è—Ç–æ—Ä—ã"]
        }
        
        related_terms = []
        for key, terms in related_terms_map.items():
            if key in topic_lower:
                related_terms.extend(terms)
        
        return related_terms[:5]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
    
    def _determine_style(self, topic: str) -> str:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å—Ç–∏–ª—è –Ω–∞–ø–∏—Å–∞–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–º—ã"""
        scientific_keywords = ["–∫–≤–∞–Ω—Ç–æ–≤—ã–µ", "–±–∏–æ—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏", "–Ω–µ–π—Ä–æ—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏", "–≥–µ–Ω–Ω–∞—è –∏–Ω–∂–µ–Ω–µ—Ä–∏—è"]
        popular_keywords = ["—Ç—É—Ä–∏–∑–º", "—É–º–Ω—ã–µ –≥–æ—Ä–æ–¥–∞", "–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ", "–±–ª–æ–∫—á–µ–π–Ω"]
        
        topic_lower = topic.lower()
        
        if any(keyword in topic_lower for keyword in scientific_keywords):
            return "–Ω–∞—É—á–Ω–æ-–ø–æ–ø—É–ª—è—Ä–Ω—ã–π"
        elif any(keyword in topic_lower for keyword in popular_keywords):
            return "–ø—É–±–ª–∏—Ü–∏—Å—Ç–∏—á–µ—Å–∫–∏–π"
        else:
            return "–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ-–∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏–π"
    
    def _determine_tone(self, topic: str) -> str:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏"""
        positive_keywords = ["–≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏", "—Ä–∞–∑–≤–∏—Ç–∏–µ", "–±—É–¥—É—â–µ–µ", "–∏–Ω–Ω–æ–≤–∞—Ü–∏–∏", "–ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤—ã"]
        neutral_keywords = ["–∞–Ω–∞–ª–∏–∑", "–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ", "—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏", "—Å–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–µ"]
        
        topic_lower = topic.lower()
        
        if any(keyword in topic_lower for keyword in positive_keywords):
            return "–Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ-–ø–æ–∑–∏—Ç–∏–≤–Ω—ã–π"
        elif any(keyword in topic_lower for keyword in neutral_keywords):
            return "–Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π"
        else:
            return "–æ–±—ä–µ–∫—Ç–∏–≤–Ω—ã–π"
    
    def _determine_audience(self, topic: str) -> str:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π –∞—É–¥–∏—Ç–æ—Ä–∏–∏"""
        if "–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ" in topic.lower():
            return "–ø–µ–¥–∞–≥–æ–≥–∏, —Å—Ç—É–¥–µ–Ω—Ç—ã, —Ä–æ–¥–∏—Ç–µ–ª–∏"
        elif "–º–µ–¥–∏—Ü–∏–Ω–∞" in topic.lower():
            return "–≤—Ä–∞—á–∏, –ø–∞—Ü–∏–µ–Ω—Ç—ã, –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏"
        elif "—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏" in topic.lower():
            return "IT-—Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç—ã, –±–∏–∑–Ω–µ—Å-–ª–∏–¥–µ—Ä—ã, —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ —ç–Ω—Ç—É–∑–∏–∞—Å—Ç—ã"
        elif "–±–∏–∑–Ω–µ—Å" in topic.lower():
            return "–ø—Ä–µ–¥–ø—Ä–∏–Ω–∏–º–∞—Ç–µ–ª–∏, –º–µ–Ω–µ–¥–∂–µ—Ä—ã, –∏–Ω–≤–µ—Å—Ç–æ—Ä—ã"
        else:
            return "—à–∏—Ä–æ–∫–∞—è –∞—É–¥–∏—Ç–æ—Ä–∏—è, –∏–Ω—Ç–µ—Ä–µ—Å—É—é—â–∞—è—Å—è —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è–º–∏"
    
    def _assess_complexity(self, topic: str) -> str:
        """–û—Ü–µ–Ω–∫–∞ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ —Ç–µ–º—ã"""
        complex_keywords = ["–∫–≤–∞–Ω—Ç–æ–≤—ã–µ", "–Ω–µ–π—Ä–æ—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏", "–≥–µ–Ω–Ω–∞—è –∏–Ω–∂–µ–Ω–µ—Ä–∏—è", "–±–ª–æ–∫—á–µ–π–Ω"]
        if any(keyword in topic.lower() for keyword in complex_keywords):
            return "–≤—ã—Å–æ–∫–∞—è"
        else:
            return "—Å—Ä–µ–¥–Ω—è—è"
    
    def generate_content(self, topic: str, analysis: Dict[str, Any]) -> Dict[str, str]:
        """3. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞ —Å –ø–æ–º–æ—â—å—é –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏"""
        logger.info("–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞...")
        
        if self.text_generator is None:
            return self._generate_fallback_content(topic, analysis)
        
        try:
            set_seed(42)  # –î–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏
            
            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∑–∞–≥–æ–ª–æ–≤–∫–∞
            title = self._generate_title(topic, analysis)
            
            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å—Ç–∞—Ç—å–∏
            article = self._generate_article(topic, analysis, title)
            
            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è meta description
            meta_description = self._generate_meta_description(topic, article)
            
            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–≥–æ–≤
            tags = self._generate_tags(topic, analysis, article)
            
            content = {
                "title": title,
                "article": article,
                "meta_description": meta_description,
                "tags": tags
            }
            
            logger.info("–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
            return content
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞: {e}")
            return self._generate_fallback_content(topic, analysis)
    
    def _generate_title(self, topic: str, analysis: Dict[str, Any]) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∑–∞–≥–æ–ª–æ–≤–∫–∞"""
        prompt = f"–°–æ–∑–¥–∞–π –∫—Ä–∞—Ç–∫–∏–π –∏ –ø—Ä–∏–≤–ª–µ–∫–∞—Ç–µ–ª—å–Ω—ã–π –∑–∞–≥–æ–ª–æ–≤–æ–∫ –Ω–∞ —Ç–µ–º—É: {topic}. –°—Ç–∏–ª—å: {analysis['style']}. –¢–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å: {analysis['tone']}."
        
        try:
            result = self.text_generator(
                prompt,
                max_length=100,
                num_return_sequences=1,
                temperature=0.7,
                do_sample=True,
                pad_token_id=50256
            )
            title = result[0]['generated_text'].replace(prompt, '').strip()
            # –û—á–∏—Å—Ç–∫–∞ –∏ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–≥–æ–ª–æ–≤–∫–∞
            title = re.split(r'[.!?]', title)[0].strip()
            return title if len(title) > 10 else f"{topic}: –Ω–æ–≤—ã–µ –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤—ã"
        except:
            return f"{topic}: –∞–Ω–∞–ª–∏–∑ –∏ –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤—ã"
    
    def _generate_article(self, topic: str, analysis: Dict[str, Any], title: str) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Å–Ω–æ–≤–Ω–æ–π —Å—Ç–∞—Ç—å–∏"""
        prompt = f"""–ù–∞–ø–∏—à–∏ —Ä–∞–∑–≤–µ—Ä–Ω—É—Ç—É—é —Å—Ç–∞—Ç—å—é –Ω–∞ —Ç–µ–º—É "{title}". 
–°—Ç–∏–ª—å: {analysis['style']}. 
–¢–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å: {analysis['tone']}.
–¶–µ–ª–µ–≤–∞—è –∞—É–¥–∏—Ç–æ—Ä–∏—è: {analysis['target_audience']}.
–ò—Å–ø–æ–ª—å–∑—É–π –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞: {', '.join(analysis['keywords'][:5])}.
–°—Ç–∞—Ç—å—è –¥–æ–ª–∂–Ω–∞ —Å–æ–¥–µ—Ä–∂–∞—Ç—å –≤–≤–µ–¥–µ–Ω–∏–µ, –æ—Å–Ω–æ–≤–Ω—É—é —á–∞—Å—Ç—å –∏ –∑–∞–∫–ª—é—á–µ–Ω–∏–µ. –û–±—ä–µ–º: 5-7 –∞–±–∑–∞—Ü–µ–≤."""

        try:
            result = self.text_generator(
                prompt,
                max_length=1500,
                num_return_sequences=1,
                temperature=0.8,
                do_sample=True,
                pad_token_id=50256
            )
            article = result[0]['generated_text'].replace(prompt, '').strip()
            
            # –ü–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å—Ç–∞—Ç—å–∏
            article = self._postprocess_article(article)
            return article
        except:
            return self._generate_fallback_article(topic, analysis)
    
    def _generate_meta_description(self, topic: str, article: str) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è meta description"""
        # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ –¥–≤–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –∏–∑ —Å—Ç–∞—Ç—å–∏ –∏–ª–∏ —Å–æ–∑–¥–∞–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ
        sentences = re.split(r'[.!?]', article)
        if len(sentences) >= 2:
            description = ' '.join(sentences[:2]).strip() + '.'
        else:
            description = f"–£–∑–Ω–∞–π—Ç–µ –±–æ–ª—å—à–µ –æ {topic.lower()} –≤ –Ω–∞—à–µ–π –ø–æ–¥—Ä–æ–±–Ω–æ–π —Å—Ç–∞—Ç—å–µ."
        
        # –û–±—Ä–µ–∑–∞–µ–º –¥–æ 160 —Å–∏–º–≤–æ–ª–æ–≤ (—Å—Ç–∞–Ω–¥–∞—Ä—Ç –¥–ª—è meta description)
        if len(description) > 160:
            description = description[:157] + '...'
        
        return description
    
    def _generate_tags(self, topic: str, analysis: Dict[str, Any], article: str) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–≥–æ–≤"""
        base_tags = analysis['keywords'][:3]  # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 3 –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤–∞
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–º—ã –∏–∑ —Å—Ç–∞—Ç—å–∏
        words = re.findall(r'\b[–∞-—è—ë]{4,}\b', article.lower())
        word_freq = {}
        for word in words:
            if word not in analysis['keywords'] and len(word) > 3:
                word_freq[word] = word_freq.get(word, 0) + 1
        
        # –ë–µ—Ä–µ–º 2 —Å–∞–º—ã—Ö —á–∞—Å—Ç—ã—Ö —Å–ª–æ–≤–∞ –∏–∑ —Å—Ç–∞—Ç—å–∏
        top_article_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:2]
        article_tags = [word for word, freq in top_article_words]
        
        all_tags = base_tags + article_tags
        return list(set(all_tags))[:5]  # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏ –∏ –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
    
    def _postprocess_article(self, article: str) -> str:
        """–ü–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Å—Ç–∞—Ç—å–∏"""
        # –£–¥–∞–ª—è–µ–º –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        sentences = re.split(r'[.!?]', article)
        unique_sentences = []
        for sentence in sentences:
            sentence = sentence.strip()
            if sentence and sentence not in unique_sentences:
                unique_sentences.append(sentence)
        
        # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –≤ –∞–±–∑–∞—Ü—ã (–∫–∞–∂–¥—ã–µ 3-4 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è - –∞–±–∑–∞—Ü)
        formatted_paragraphs = []
        for i in range(0, len(unique_sentences), 3):
            paragraph = '. '.join(unique_sentences[i:i+3]) + '.'
            formatted_paragraphs.append(paragraph)
        
        return '\n\n'.join(formatted_paragraphs)
    
    def _generate_fallback_content(self, topic: str, analysis: Dict[str, Any]) -> Dict[str, str]:
        """–†–µ–∑–µ—Ä–≤–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –µ—Å–ª–∏ –º–æ–¥–µ–ª—å –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞"""
        logger.warning("–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Ä–µ–∑–µ—Ä–≤–Ω–æ–≥–æ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞")
        
        title = f"{topic}: –∞–Ω–∞–ª–∏–∑ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ç–µ–Ω–¥–µ–Ω—Ü–∏–π"
        
        article_template = """
–í–≤–µ–¥–µ–Ω–∏–µ: –¢–µ–º–∞ {topic} —Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è –≤—Å–µ –±–æ–ª–µ–µ –∞–∫—Ç—É–∞–ª—å–Ω–æ–π –≤ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–º –º–∏—Ä–µ. 
–≠—Ç–æ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–µ –∞—Å–ø–µ–∫—Ç—ã —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ –∏ —Å–æ—Ü–∏–∞–ª—å–Ω–æ–≥–æ —Ä–∞–∑–≤–∏—Ç–∏—è.

–û—Å–Ω–æ–≤–Ω–∞—è —á–∞—Å—Ç—å: –°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ —Ä–∞–∑–≤–∏—Ç–∏–µ –≤ –æ–±–ª–∞—Å—Ç–∏ {keywords} –æ—Ç–∫—Ä—ã–≤–∞–µ—Ç –Ω–æ–≤—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏. 
–ú–Ω–æ–≥–∏–µ —ç–∫—Å–ø–µ—Ä—Ç—ã —Å—Ö–æ–¥—è—Ç—Å—è –≤–æ –º–Ω–µ–Ω–∏–∏, —á—Ç–æ —ç—Ç–æ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –±—É–¥–µ—Ç –æ–ø—Ä–µ–¥–µ–ª—è—Ç—å –±—É–¥—É—â–µ–µ —Ä–∞–∑–≤–∏—Ç–∏–µ. 
–¢–µ—Ö–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –ø—Ä–æ–≥—Ä–µ—Å—Å –ø–æ–∑–≤–æ–ª—è–µ—Ç —Ä–µ—à–∞—Ç—å –∑–∞–¥–∞—á–∏, –∫–æ—Ç–æ—Ä—ã–µ —Ä–∞–Ω–µ–µ –∫–∞–∑–∞–ª–∏—Å—å –Ω–µ–≤–æ–∑–º–æ–∂–Ω—ã–º–∏.

–ó–∞–∫–ª—é—á–µ–Ω–∏–µ: –¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, {topic} –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤–Ω–æ–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ –∏–∑—É—á–µ–Ω–∏—è –∏ —Ä–∞–∑–≤–∏—Ç–∏—è. 
–ù–µ–æ–±—Ö–æ–¥–∏–º–æ –ø—Ä–æ–¥–æ–ª–∂–∞—Ç—å –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –≤ —ç—Ç–æ–π –æ–±–ª–∞—Å—Ç–∏ –¥–ª—è –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ —ç—Ñ—Ñ–µ–∫—Ç–∞.
""".format(
    topic=topic,
    keywords=', '.join(analysis['keywords'][:3])
)

        meta_description = f"–ê–Ω–∞–ª–∏–∑ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ç–µ–Ω–¥–µ–Ω—Ü–∏–π –∏ –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤ —Ä–∞–∑–≤–∏—Ç–∏—è –≤ –æ–±–ª–∞—Å—Ç–∏ {topic.lower()}."
        
        tags = analysis['keywords'][:4] + [topic.split(':')[0].strip()]
        
        return {
            "title": title,
            "article": article_template.strip(),
            "meta_description": meta_description,
            "tags": tags
        }
    
    def _generate_fallback_article(self, topic: str, analysis: Dict[str, Any]) -> str:
        """–†–µ–∑–µ—Ä–≤–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Å—Ç–∞—Ç—å–∏"""
        return f"""
–°—Ç–∞—Ç—å—è –ø–æ—Å–≤—è—â–µ–Ω–∞ –∞–Ω–∞–ª–∏–∑—É —Ç–µ–º—ã "{topic}". –í —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–º –º–∏—Ä–µ —ç—Ç–∞ –æ–±–ª–∞—Å—Ç—å –ø—Ä–∏–æ–±—Ä–µ—Ç–∞–µ—Ç –≤—Å–µ –±–æ–ª—å—à–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ.

–ö–ª—é—á–µ–≤—ã–µ –∞—Å–ø–µ–∫—Ç—ã, —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ–º—ã–µ –≤ —Å—Ç–∞—Ç—å–µ:
- –û—Å–Ω–æ–≤–Ω—ã–µ —Ç–µ–Ω–¥–µ–Ω—Ü–∏–∏ —Ä–∞–∑–≤–∏—Ç–∏—è
- –¢–µ—Ö–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ –∏–Ω–Ω–æ–≤–∞—Ü–∏–∏
- –ü–µ—Ä—Å–ø–µ–∫—Ç–∏–≤—ã –Ω–∞ –±—É–¥—É—â–µ–µ

–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π –≤ –¥–∞–Ω–Ω–æ–π —Å—Ñ–µ—Ä–µ –ø–æ–∑–≤–æ–ª—è–µ—Ç –¥–æ—Å—Ç–∏—á—å –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤. 
–≠–∫—Å–ø–µ—Ä—Ç—ã –æ—Ç–º–µ—á–∞—é—Ç —Ä–æ—Å—Ç –∏–Ω—Ç–µ—Ä–µ—Å–∞ –∫ —ç—Ç–æ–º—É –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—é —Å–æ —Å—Ç–æ—Ä–æ–Ω—ã –∫–∞–∫ –Ω–∞—É—á–Ω–æ–≥–æ —Å–æ–æ–±—â–µ—Å—Ç–≤–∞, —Ç–∞–∫ –∏ –±–∏–∑–Ω–µ—Å–∞.

–í –∑–∞–∫–ª—é—á–µ–Ω–∏–µ –º–æ–∂–Ω–æ –æ—Ç–º–µ—Ç–∏—Ç—å, —á—Ç–æ –¥–∞–ª—å–Ω–µ–π—à–µ–µ —Ä–∞–∑–≤–∏—Ç–∏–µ —Ç–µ–º—ã "{topic}" –±—É–¥–µ—Ç —Å–ø–æ—Å–æ–±—Å—Ç–≤–æ–≤–∞—Ç—å –ø—Ä–æ–≥—Ä–µ—Å—Å—É –≤ —Å–º–µ–∂–Ω—ã—Ö –æ–±–ª–∞—Å—Ç—è—Ö 
–∏ –æ—Ç–∫—Ä—ã–≤–∞—Ç—å –Ω–æ–≤—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –¥–ª—è –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π –∏ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è.
""".strip()
    
    def validate_content(self, content: Dict[str, str], analysis: Dict[str, Any]) -> Dict[str, Any]:
        """4. –í–∞–ª–∏–¥–∞—Ü–∏—è —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        logger.info("–í–∞–ª–∏–¥–∞—Ü–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞...")
        
        article = content["article"]
        title = content["title"]
        meta_description = content["meta_description"]
        
        validation_results = {
            "length_validation": self._validate_length(content),
            "keywords_validation": self._validate_keywords(article, analysis["keywords"]),
            "tone_validation": self._validate_tone(article, analysis["tone"]),
            "readability_validation": self._validate_readability(article),
            "seo_validation": self._validate_seo(content)
        }
        
        # –û–±—â–∏–π —Å—Ç–∞—Ç—É—Å
        overall_status = "valid"
        for check_name, check_result in validation_results.items():
            if not check_result["status"]:
                overall_status = "needs_improvement"
                break
        
        validation_results["overall_status"] = overall_status
        
        logger.info(f"–í–∞–ª–∏–¥–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –û–±—â–∏–π —Å—Ç–∞—Ç—É—Å: {overall_status}")
        return validation_results
    
    def _validate_length(self, content: Dict[str, str]) -> Dict[str, Any]:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–ª–∏–Ω—ã –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        article_len = len(content["article"])
        title_len = len(content["title"])
        meta_len = len(content["meta_description"])
        
        requirements = {
            "article": {"min": 500, "max": 3000},
            "title": {"min": 20, "max": 80},
            "meta_description": {"min": 50, "max": 160}
        }
        
        article_ok = requirements["article"]["min"] <= article_len <= requirements["article"]["max"]
        title_ok = requirements["title"]["min"] <= title_len <= requirements["title"]["max"]
        meta_ok = requirements["meta_description"]["min"] <= meta_len <= requirements["meta_description"]["max"]
        
        return {
            "status": article_ok and title_ok and meta_ok,
            "details": {
                "article_length": article_len,
                "title_length": title_len,
                "meta_description_length": meta_len,
                "article_requirements": f"{requirements['article']['min']}-{requirements['article']['max']}",
                "title_requirements": f"{requirements['title']['min']}-{requirements['title']['max']}",
                "meta_requirements": f"{requirements['meta_description']['min']}-{requirements['meta_description']['max']}"
            }
        }
    
    def _validate_keywords(self, article: str, keywords: List[str]) -> Dict[str, Any]:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤"""
        article_lower = article.lower()
        found_keywords = []
        missing_keywords = []
        
        for keyword in keywords[:8]:  # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–µ—Ä–≤—ã–µ 8 –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
            if keyword.lower() in article_lower:
                found_keywords.append(keyword)
            else:
                missing_keywords.append(keyword)
        
        coverage = len(found_keywords) / len(keywords[:8]) if keywords else 0
        status = coverage >= 0.5  # –¢—Ä–µ–±—É–µ–º —Ö–æ—Ç—è –±—ã 50% –ø–æ–∫—Ä—ã—Ç–∏—è
        
        return {
            "status": status,
            "details": {
                "total_keywords_checked": len(keywords[:8]),
                "found_keywords": found_keywords,
                "missing_keywords": missing_keywords,
                "coverage_percentage": round(coverage * 100, 1)
            }
        }
    
    def _validate_tone(self, article: str, expected_tone: str) -> Dict[str, Any]:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏"""
        # –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏
        positive_words = ["—É—Å–ø–µ—Ö", "–ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤–∞", "—Ä–∞–∑–≤–∏—Ç–∏–µ", "–∏–Ω–Ω–æ–≤–∞—Ü–∏—è", "–ø—Ä–æ–≥—Ä–µ—Å—Å"]
        negative_words = ["–ø—Ä–æ–±–ª–µ–º–∞", "—Ä–∏—Å–∫", "–æ–ø–∞—Å–Ω–æ—Å—Ç—å", "—É–≥—Ä–æ–∑–∞", "—Å–ª–æ–∂–Ω–æ—Å—Ç—å"]
        
        article_lower = article.lower()
        positive_count = sum(1 for word in positive_words if word in article_lower)
        negative_count = sum(1 for word in negative_words if word in article_lower)
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ñ–∞–∫—Ç–∏—á–µ—Å–∫—É—é —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å
        if positive_count > negative_count + 2:
            actual_tone = "–ø–æ–∑–∏—Ç–∏–≤–Ω—ã–π"
        elif negative_count > positive_count + 2:
            actual_tone = "–Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–π"
        else:
            actual_tone = "–Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π"
        
        # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º —Å –æ–∂–∏–¥–∞–µ–º–æ–π
        tone_mapping = {
            "–Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ-–ø–æ–∑–∏—Ç–∏–≤–Ω—ã–π": ["–Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π", "–ø–æ–∑–∏—Ç–∏–≤–Ω—ã–π"],
            "–Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π": ["–Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π"],
            "–æ–±—ä–µ–∫—Ç–∏–≤–Ω—ã–π": ["–Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π", "–ø–æ–∑–∏—Ç–∏–≤–Ω—ã–π"]
        }
        
        expected_tones = tone_mapping.get(expected_tone, ["–Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π"])
        status = actual_tone in expected_tones
        
        return {
            "status": status,
            "details": {
                "expected_tone": expected_tone,
                "detected_tone": actual_tone,
                "positive_words_count": positive_count,
                "negative_words_count": negative_count
            }
        }
    
    def _validate_readability(self, article: str) -> Dict[str, Any]:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —á–∏—Ç–∞–µ–º–æ—Å—Ç–∏"""
        sentences = re.split(r'[.!?]', article)
        sentences = [s.strip() for s in sentences if s.strip()]
        
        words = re.findall(r'\b[–∞-—è—ë]+\b', article.lower())
        
        if sentences and words:
            avg_sentence_length = len(words) / len(sentences)
            avg_word_length = sum(len(word) for word in words) / len(words)
        else:
            avg_sentence_length = 0
            avg_word_length = 0
        
        # –ö—Ä–∏—Ç–µ—Ä–∏–∏ —á–∏—Ç–∞–µ–º–æ—Å—Ç–∏
        sentence_length_ok = 10 <= avg_sentence_length <= 25
        word_length_ok = 4 <= avg_word_length <= 8
        
        return {
            "status": sentence_length_ok and word_length_ok,
            "details": {
                "avg_sentence_length": round(avg_sentence_length, 1),
                "avg_word_length": round(avg_word_length, 1),
                "total_sentences": len(sentences),
                "total_words": len(words)
            }
        }
    
    def _validate_seo(self, content: Dict[str, str]) -> Dict[str, Any]:
        """SEO-–≤–∞–ª–∏–¥–∞—Ü–∏—è"""
        title = content["title"]
        article = content["article"]
        meta_description = content["meta_description"]
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –≤ –∑–∞–≥–æ–ª–æ–≤–∫–µ –∏ –Ω–∞—á–∞–ª–µ —Å—Ç–∞—Ç—å–∏
        first_paragraph = article[:200].lower()
        
        issues = []
        
        if len(title) < 20:
            issues.append("–ó–∞–≥–æ–ª–æ–≤–æ–∫ —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π")
        if len(title) > 80:
            issues.append("–ó–∞–≥–æ–ª–æ–≤–æ–∫ —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–π")
        
        if len(meta_description) < 50:
            issues.append("Meta description —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π")
        if len(meta_description) > 160:
            issues.append("Meta description —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–π")
        
        if len(article) < 1000:
            issues.append("–°—Ç–∞—Ç—å—è —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∞—è –¥–ª—è —Ö–æ—Ä–æ—à–µ–≥–æ SEO")
        
        return {
            "status": len(issues) == 0,
            "details": {
                "issues": issues,
                "seo_score": max(0, 10 - len(issues))
            }
        }
    
    def generate_complete_report(self) -> Dict[str, Any]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–æ–ª–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞"""
        logger.info("–ó–∞–ø—É—Å–∫ –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–π —Å–∏—Å—Ç–µ–º—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞...")
        
        # –®–∞–≥ 1: –í—ã–±–æ—Ä —Ç–µ–º—ã
        topic = self.select_topic()
        
        # –®–∞–≥ 2: –ê–Ω–∞–ª–∏–∑ —Ç–µ–º—ã
        analysis = self.analyze_topic(topic)
        
        # –®–∞–≥ 3: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        content = self.generate_content(topic, analysis)
        
        # –®–∞–≥ 4: –í–∞–ª–∏–¥–∞—Ü–∏—è
        validation = self.validate_content(content, analysis)
        
        # –®–∞–≥ 5: –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç—á–µ—Ç–∞
        report = {
            "metadata": {
                "system_name": "LocalContentGenerator",
                "model_used": self.model_name,
                "generation_date": datetime.now().isoformat(),
                "topic_type": "–°–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∞—è —Ç–µ–º–∞"
            },
            "topic_selection": {
                "selected_topic": topic,
                "available_topics_count": len(self.topics)
            },
            "content_analysis": analysis,
            "generated_content": content,
            "content_validation": validation,
            "export_info": {
                "format": "JSON",
                "encoding": "UTF-8",
                "version": "1.0"
            }
        }
        
        logger.info("–ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –∑–∞–≤–µ—Ä—à–∏–ª–∞ —Ä–∞–±–æ—Ç—É")
        return report
    
    def save_report(self, report: Dict[str, Any], filename: str = None) -> str:
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç—á–µ—Ç–∞ –≤ JSON —Ñ–∞–π–ª"""
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"content_report_{timestamp}.json"
        
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(report, f, ensure_ascii=False, indent=2)
            
            logger.info(f"–û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ —Ñ–∞–π–ª: {filename}")
            return filename
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –æ—Ç—á–µ—Ç–∞: {e}")
            return None

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ —Ä–∞–±–æ—Ç—ã —Å–∏—Å—Ç–µ–º—ã"""
    print("=== –ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ ===")
    print("–õ–æ–∫–∞–ª—å–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π")
    print("=" * 60)
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞
    generator = LocalContentGenerator()
    
    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–æ–ª–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞
    print("üîÑ –ó–∞–ø—É—Å–∫ –ø—Ä–æ—Ü–µ—Å—Å–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞...")
    report = generator.generate_complete_report()
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç—á–µ—Ç–∞
    filename = generator.save_report(report)
    
    # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    print("\n‚úÖ –ü—Ä–æ—Ü–µ—Å—Å –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ!")
    print(f"üìÑ –û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {filename}")
    print(f"üìù –¢–µ–º–∞: {report['topic_selection']['selected_topic']}")
    print(f"üìä –°—Ç–∞—Ç—É—Å –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {report['content_validation']['overall_status']}")
    
    # –î–µ—Ç–∞–ª–∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
    validation = report['content_validation']
    print(f"üìè –î–ª–∏–Ω–∞ —Å—Ç–∞—Ç—å–∏: {validation['length_validation']['details']['article_length']} —Å–∏–º–≤–æ–ª–æ–≤")
    print(f"üîë –ü–æ–∫—Ä—ã—Ç–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤: {validation['keywords_validation']['details']['coverage_percentage']}%")
    print(f"üòä –¢–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å: {validation['tone_validation']['details']['detected_tone']}")
    
    # –í—ã–≤–æ–¥ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
    content = report['generated_content']
    print(f"\nüéØ –ó–∞–≥–æ–ª–æ–≤–æ–∫: {content['title']}")
    print(f"üìã Meta Description: {content['meta_description']}")
    print(f"üè∑Ô∏è –¢–µ–≥–∏: {', '.join(content['tags'])}")
    
    print(f"\nüìñ –°—Ç–∞—Ç—å—è: {content['article']}...")
    
    return report

if __name__ == "__main__":
    # –ó–∞–ø—É—Å–∫ —Å–∏—Å—Ç–µ–º—ã
    report = main()